{
 "metadata": {
  "name": "",
  "signature": "sha256:6bbbd73d3ba39a6bf950ead45a7ecdca2caa55dc02c34b7c127d2f7e3e83c53e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# CS579: Lecture 10  \n",
      "\n",
      "**Sentiment Analysis**\n",
      "\n",
      "*[Dr. Aron Culotta](http://cs.iit.edu/~culotta)*  \n",
      "*[Illinois Institute of Technology](http://iit.edu)*\n",
      "\n",
      "*Warning: Live Twitter data is used below, some of which may be offensive.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**sen\u00b7ti\u00b7ment**\n",
      "\n",
      "1. a view of or attitude toward a situation or event; an opinion.\n",
      "2. a feeling or emotion.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**sen\u00b7ti\u00b7ment &nbsp; a\u00b7nal\u00b7y\u00b7sis ** \n",
      "\n",
      "1. classification of documents/messages by sentiment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Examples"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![tmnt.png](tmnt.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![ufo](ufo.png)\n",
      "\n",
      "![ufo-review](ufo-review.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![mcds](mcds.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "![rauner](rauner.png)\n",
      "![quinn](quinn.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "## Why is this hard?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Two Approaches\n",
      "\n",
      "- Lexicons (word lists)\n",
      "\n",
      "- Machine learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Lexicons\n",
      "\n",
      "- List of terms with positive/negative/neutral sentiment\n",
      "\n",
      "- E.g., AFINN: http://neuro.imm.dtu.dk/wiki/AFINN\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download the AFINN lexicon, unzip, and read the latest word list in AFINN-111.txt\n",
      "from StringIO import StringIO\n",
      "from zipfile import ZipFile\n",
      "from urllib import urlopen\n",
      "\n",
      "url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
      "zipfile = ZipFile(StringIO(url.read()))\n",
      "afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])\n",
      "\n",
      "print 'read', len(afinn), 'AFINN terms.\\nE.g.:', afinn.items()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 2462 AFINN terms.\n",
        "E.g.: [('limited', -1), ('suicidal', -2), ('pardon', 2), ('desirable', 2), ('protest', -2), ('lurking', -1), ('controversial', -2), ('hating', -3), ('ridiculous', -3), ('hate', -3)]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What is the distribution of scores?\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "counts = Counter(afinn.values())\n",
      "vals = sorted(counts.keys())\n",
      "\n",
      "plt.bar(vals, counts.values())\n",
      "plt.xticks(vals)\n",
      "plt.xlabel('sentiment')\n",
      "plt.ylabel('count')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEPCAYAAACp/QjLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10d6e89d0>"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# How do we score a document?\n",
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            print '\\t%s=%d' % (t, afinn[t])\n",
      "            total += afinn[t]\n",
      "    return total\n",
      "    \n",
      "doc = \"i don't know if this is a scam or if mine was broken\".split()\n",
      "print 'AFINN:\\n', afinn_sentiment(doc, afinn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AFINN:\n",
        "\tscam=-2\n",
        "\tbroken=-1\n",
        "-3.0\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What if mixed sentiment?\n",
      "doc = \"it has a hokey plot that is both to good and bad\".split()\n",
      "print 'AFINN:\\n', afinn_sentiment(doc, afinn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AFINN:\n",
        "\tgood=3\n",
        "\tbad=-3\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Distinguish neutral from pos/neg.\n",
      "# Return two scores per document.\n",
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += -1 * afinn[t]\n",
      "    return pos, neg\n",
      "\n",
      "doc = \"it has a hokey plot that is both to good and bad\".split()\n",
      "print 'AFINN:\\n', afinn_sentiment2(doc, afinn, verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AFINN:\n",
        "\tgood=3\n",
        "\tbad=-3\n",
        "(3, 3)\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI\n",
      "\n",
      "# Get some tweets about McDonald's\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    \"\"\" Read the config_file and construct an instance of TwitterAPI.\n",
      "    Args:\n",
      "      config_file ... A config file in ConfigParser format with Twitter credentials\n",
      "    Returns:\n",
      "      An instance of TwitterAPI.\n",
      "    \"\"\"\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "\n",
      "twitter = get_twitter('twitter.cfg')\n",
      "tweets = []\n",
      "for r in twitter.request('search/tweets', {'q': 'mcdonalds', 'count': 100}):\n",
      "    tweets.append(r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'read %d mcdonalds tweets' % len(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "read 100 mcdonalds tweets\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Tokenize tweets\n",
      "import re\n",
      "\n",
      "def tokenize(text):\n",
      "    return re.sub('\\W+', ' ', text.lower()).split()\n",
      "\n",
      "tokens = [tokenize(t['text']) for t in tweets]\n",
      "print 'tokenized, e.g., \\n%s\\nto\\n%s' % (tweets[0]['text'], tokens[0])   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tokenized, e.g., \n",
        "RT @WorldStarFunny: When you're high AF and you tryna order your food at McDonalds http://t.co/WkfqDw1y26\n",
        "to\n",
        "[u'rt', u'worldstarfunny', u'when', u'you', u're', u'high', u'af', u'and', u'you', u'tryna', u'order', u'your', u'food', u'at', u'mcdonalds', u'http', u't', u'co', u'wkfqdw1y26']\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "positives = []\n",
      "negatives = []\n",
      "for tweet in tokens:\n",
      "    pos, neg = afinn_sentiment2(tweet, afinn)\n",
      "    if pos > neg:\n",
      "        positives.append((' '.join(tweet), pos, neg))\n",
      "    elif neg > pos:\n",
      "        negatives.append((' '.join(tweet), pos, neg))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print top positives:\n",
      "for tweet, pos, neg in sorted(positives, key=lambda x: x[1], reverse=True):\n",
      "    print pos, neg, tweet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8 0 rt mccafe gma we re celebrating nat l coffeeday all year long select followers will win a month of free coffee details http t co y\n",
        "6 3 i feel like going to mcdonalds for a salad is like going to a brothel for hugs if you are going to eat badly http t co v52et8pxn8\n",
        "6 0 rt mccafe we re celebrating coffeeday by showing love to our mccafefriends all year long rules http t co yfhtmhr2da http t co x9y\n",
        "6 0 haha i m easily impressed mcdonalds http t co wkkv5oooaa\n",
        "5 0 kingcarnivore_ mcdonalds do you guys even care your food is trash and the movie super size me proved that\n",
        "4 0 trips to mcdonalds are always fun\n",
        "4 0 a13stovall once again noooo food in this house i need a ride to mcdonalds lmao crisissituation yesimfat i had some earlier\n",
        "4 0 national coffee day go to dunkin donuts and mcdonalds for free coffee if you drink it lol\n",
        "4 1 rt mcdonalds 1 4 odds of winning means training could really pay off mcdmonopoly starts 9 30 rules http t co qmt36e19wn http t co\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 0 rt lrsherwood free coffee at quick trip krispy kreme mcdonalds dunkin rt to save a life\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 0 rt y0itsdellz just to let everyone know mcdonalds will be selling pizza burgers from 5th 12th nov and i m very excited\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 0 rt y0itsdellz just to let everyone know mcdonalds will be selling pizza burgers from 5th 12th nov and i m very excited\n",
        "3 0 rt y0itsdellz just to let everyone know mcdonalds will be selling pizza burgers from 5th 12th nov and i m very excited\n",
        "3 0 rt geektyrant hong kong mcdonalds has a batman burger meal http t co x8wndgplug popular via geektyrant http t co fk7q1xf5gp\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 1 rt berniematthew the nice guy at mcdonalds let me get breakfast 6 minutes after the cut off time this morning this is what it must feel\n",
        "3 0 rt mcdonalds the game you love is back amp we brought company check out who s joining mcdmonopoly rules http t co dsrexgk8rp\n",
        "2 0 rt ayeyotreydoee she look like she sponsoring mcdonalds http t co 7pqhijluhg\n",
        "2 0 rt gmel_genuine nicki minaj look like she fell in a mcdonalds play pin http t co usbawupbjr\n",
        "2 0 mcdonalds responsible for treatment of workers agency says\n",
        "1 0 rt courtneygrierx nashhiv omg i rlly want mcdonalds now\n",
        "1 0 nashhiv omg i rlly want mcdonalds now\n",
        "1 0 billglockman mcdonalds i paid 8 to only throw it up quit playing and refund my money with interest mcdonald s\n",
        "1 0 rt _queenabby__ i want mcdonalds\n",
        "1 0 i want mcdonalds cookies\n",
        "1 0 courtneyedgell its k we ll put it up for adoption im sure mcdonalds will want it\n",
        "1 0 rt mccafe time free coffee doesn t end today for mccafefriends each weekday one follower gets a month of coffee details http t co\n",
        "1 0 i want mcdonalds\n",
        "1 0 rt jasmine_blu today is national coffe day you can get free coffee from mcdonalds small dunkin doughnuts medium krispy kreme\n",
        "1 0 i can t believe people actually just accept mcdonalds food as a part of their lives\n",
        "1 0 when u want mcdonalds breakfast but it s 2 late\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print afinn['accept']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print top negatives:\n",
      "for tweet, pos, neg in sorted(negatives, key=lambda x: x[2], reverse=True):\n",
      "    print pos, neg, tweet"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 8 rt kiarithestone of course human meat was found in mcdonalds niggas been tryna tell you for years but it was just another conspiracy th\n",
        "0 8 of course human meat was found in mcdonalds niggas been tryna tell you for years but it was just another conspiracy theory\n",
        "4 6 rt imneverchillin niggas drop outta high school amp get a job at mcdonalds talkin bout on my grind lmao okay grind me up a oreo mcflurr\n",
        "3 6 bastosssss_ lol i don t even eat mcdonalds anymore i ll be sick as hell\n",
        "3 6 615retro bruh i hate the mcdonalds in old hickory mfs give me these hand me down fries everytime i hate mcdonald s as a whole lol\n",
        "4 6 rt imneverchillin niggas drop outta high school amp get a job at mcdonalds talkin bout on my grind lmao okay grind me up a oreo mcflurr\n",
        "0 6 rt bonghitsandlace fucksieker mcdonalds is delicious http t co 9apwzvajdn makes me fucking sick\n",
        "0 6 https t co 6egez4ueef the worst is nobody is surprise about it are we puppets mcdonalds fake\n",
        "3 6 rt a_dreezyy1 thedrezzydre i love you gross fuckers mcdonalds thejwhitway\n",
        "0 5 rt __panache__ loyaltyblondie bitches got that lv bag bt be eating at mcdonalds gtfoh with cheap body spray inside and a few wrinkle\n",
        "0 4 was about 5 seconds from getting mcdonalds for lunch then i saw this obese lady walk out eating fries ran my ass to subway amp got a salad\n",
        "0 3 im going to die because of mcdonalds\n",
        "0 3 every time i eat mcdonalds i feel so guilty afterwards\n",
        "0 3 rt iamwillemdafoe mcdonalds is fuckin up so bad by only serving breakfast in the mornings and not serving anything else in the mornings\n",
        "0 2 i swear i m addicted to the caramel iced frapp form mcdonalds\n",
        "0 2 mcdonalds made my stomach hurt\n",
        "0 2 mcdonalds these mcgriddle s are so tits today\n",
        "0 1 rt ashtonsbatman icreepon5sos my country has no netflix mcdonalds and starbucks where do you live omg\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Sentiment Trends?\n",
      "![politico](politico.png)\n",
      "\n",
      "What can go wrong?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Which words contribute most to sentiment?\n",
      "all_counts = Counter()\n",
      "for tweet in tokens:\n",
      "    all_counts.update(tweet)\n",
      "sorted_tokens = sorted(all_counts.items(), key=lambda x:x[1], reverse=True)\n",
      "i = 0\n",
      "for token, count in sorted_tokens:\n",
      "    if token in afinn:\n",
      "        print '%s count=%d sentiment=%d' % (token, count, afinn[token])\n",
      "        i += 1\n",
      "        if i > 10:\n",
      "            break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "want count=7 sentiment=1\n",
        "cut count=6 sentiment=-1\n",
        "nice count=6 sentiment=3\n",
        "free count=5 sentiment=1\n",
        "niggas count=4 sentiment=-5\n",
        "like count=4 sentiment=2\n",
        "love count=3 sentiment=3\n",
        "lmao count=3 sentiment=4\n",
        "excited count=3 sentiment=3\n",
        "lol count=3 sentiment=3\n",
        "hate count=2 sentiment=-3\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Part of speech?\n",
      "\n",
      "> *I hate McDonlads.*  \n",
      "> *Hate speech is wrong.*\n",
      "\n",
      "> *Rauner is lying.*  \n",
      "> *Lying in bed with the flu.*\n",
      "\n",
      "Noun/verb/adjective form of term may have different sentiment."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# WordNet\n",
      "\n",
      "- A database of words and their relations\n",
      "- http://wordnet.princeton.edu/\n",
      "- Statistics: http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# WordNet\n",
      "\n",
      "- **Synset:**\n",
      "  - A list of synonyms\n",
      "  - E.g. [dog](http://wordnetweb.princeton.edu/perl/webwn?s=dog&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=11000000000000000000001000000000)\n",
      "- **Hypernym:**\n",
      "  - Y is a Hypernym of X if every X is a kind of Y\n",
      "    - **canine** is a hypernym of **dog**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SentiWordNet\n",
      "\n",
      "http://sentiwordnet.isti.cnr.it/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import sentiwordnet as swn\n",
      "# Depends on nltk (pip install nltk)\n",
      "# See http://www.nltk.org/data.html\n",
      "happy = swn.senti_synsets('happy', 'a')[0]\n",
      "print happy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<happy.a.01: PosScore=0.875 NegScore=0.0>\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sad = swn.senti_synsets('sad', 'a')[0]\n",
      "print sad"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<sad.a.01: PosScore=0.125 NegScore=0.75>\n"
       ]
      }
     ],
     "prompt_number": 79
    }
   ],
   "metadata": {}
  }
 ]
}