{
 "metadata": {
  "name": "",
  "signature": "sha256:29e38ab2de3949f6acd451894c7d7d0d65f9c921e86ea7354e98fc8ee1118e19"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CS579: Lecture 13  \n",
      "\n",
      "**Demographic Inference II**\n",
      "\n",
      "*[Dr. Aron Culotta](http://cs.iit.edu/~culotta)*  \n",
      "*[Illinois Institute of Technology](http://iit.edu)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gender Classification\n",
      "\n",
      "Let's build a classifier to predict whether a Twitter user is male/female.\n",
      "\n",
      "We'll collect \"labeled\" training data using Census name list."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1.) Collect Census names. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()\n",
      "print 'male name sample:', list(male_names)[:5]\n",
      "print 'female name sample:', list(female_names)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "male name sample: [u'trenton', u'darrin', u'emile', u'jason', u'ron']\n",
        "female name sample: [u'fawn', u'kymberly', u'augustina', u'evalyn', u'chieko']\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2.) Sample 5K tweets with names on the Census list. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct TwitterAPI object.\n",
      "\n",
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "\n",
      "twitter = get_twitter('twitter.cfg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sample U.S. tweets with names from Census. \n",
      "import sys\n",
      "\n",
      "def get_first_name(tweet):\n",
      "    if 'user' in tweet and 'name' in tweet['user']:\n",
      "        parts = tweet['user']['name'].split()\n",
      "        if len(parts) > 0:\n",
      "            return parts[0].lower()\n",
      "\n",
      "def sample_tweets(twitter, limit, male_names, female_names):\n",
      "    tweets = []\n",
      "    while True:\n",
      "        try:\n",
      "            # Restrict to U.S.\n",
      "            for response in twitter.request('statuses/filter',\n",
      "                        {'locations':'-124.637,24.548,-66.993,48.9974'}):\n",
      "                if 'user' in response:\n",
      "                    name = get_first_name(response)\n",
      "                    if name in male_names or name in female_names:\n",
      "                        tweets.append(response)\n",
      "                        if len(tweets) % 100 == 0:\n",
      "                            print 'found %d tweets' % len(tweets)\n",
      "                        if len(tweets) >= limit:\n",
      "                            return tweets\n",
      "        except:\n",
      "            print \"Unexpected error:\", sys.exc_info()[0]\n",
      "    return tweets\n",
      "        \n",
      "tweets = sample_tweets(twitter, 5000, male_names, female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "found 100 tweets\n",
        "found 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unexpected error:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <class 'requests.exceptions.ChunkedEncodingError'>\n",
        "found 3300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unexpected error:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <class 'requests.exceptions.ChunkedEncodingError'>\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "print 'sampled %d tweets' % len(tweets)\n",
      "print 'top names:', Counter(get_first_name(t) for t in tweets).most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sampled 6704 tweets\n",
        "top names: [(u'michael', 62), (u'nick', 55), (u'jay', 52), (u'justin', 51), (u'amanda', 50), (u'mike', 50), (u'john', 49), (u'chris', 48), (u'ryan', 47), (u'steve', 46)]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save these tweets.\n",
      "import pickle\n",
      "pickle.dump(tweets, open('tweets.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3.) Tokenize tweets. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tweet = tweets[7]\n",
      "print 'test tweet:\\n\\tdescr=%s\\n\\ttext=%s' % (test_tweet['user']['description'],\n",
      "                                              test_tweet['text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test tweet:\n",
        "\tdescr=Supercomplicated, goofy, Catholic, future tv personality, Hokie, runner, musician http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ\n",
        "\ttext=@JakesPartner thanks prayer partner. I really needed that because I've been having a really rough week.\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(tweet, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True):\n",
      "    tokens = tokenize(tweet['text'], lowercase, keep_punctuation, None,\n",
      "                       collapse_urls, collapse_mentions)\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(tweet['user']['description'], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 225
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for enumerating all possible arguments of tweet2tokens\n",
      "# https://docs.python.org/2/library/itertools.html#itertools.product\n",
      "from itertools import product\n",
      "\n",
      "use_descr_opts = [True, False]\n",
      "lowercase_opts = [True, False]\n",
      "keep_punctuation_opts = [True, False]\n",
      "descr_prefix_opts = ['d=', '']\n",
      "url_opts = [True, False]\n",
      "mention_opts = [True, False]\n",
      "\n",
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts)\n",
      "for options in option_iter:\n",
      "    print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "    print\n",
      "    print '    '.join(tweet2tokens(test_tweet, *options)), '\\n----\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in tweets]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 226
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokens_list[7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'THIS_IS_A_MENTION', u'thanks', u'prayer', u'partner', u'i', u'really', u'needed', u'that', u'because', u'i', u've', u'been', u'having', u'a', u'really', u'rough', u'week', u'd=supercomplicated', u'd=goofy', u'd=catholic', u'd=future', u'd=tv', u'd=personality', u'd=hokie', u'd=runner', u'd=musician', u'd=THIS_IS_A_URL']\n"
       ]
      }
     ],
     "prompt_number": 227
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "\n",
      "#1) Create a vocabulary (dict from term->index)\n",
      "\n",
      "# https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 228
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21618 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31446 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 119
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation and urls?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=False, collapse_mentions=True)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33137 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation and urls and mentions?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=False, collapse_mentions=False)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37352 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 121
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Vector Matrix\n",
      "\n",
      "Create a matrix $X$ where $X[i,j]$ is the frequenct of term $j$ in tweet $i$.\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sparse Matrices\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "$X$ is mostly $0$ for text problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## List of List (LIL) Matrix\n",
      "\n",
      "Store a linked list of (index, value) pairs for each row.\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "\\hbox{tweet}_1 & (0, 1)\\\\\n",
      "\\hbox{tweet}_2 & (3,2)\\\\\n",
      "\\hbox{tweet}_3 & (0,1), (1,1)\\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "**Advantage:** Fast to construct: append to list in constant time.\n",
      "\n",
      "**Disadvantage:** Slow random access for matrix-vector product.\n",
      "\n",
      "E.g., $\\hat{z} = X\\cdot \\hat{\\beta}$ to classify tweets using a learned weight vector $\\beta$\n",
      "\n",
      "$\\hat{z}[i] = \\sum_j X[i,j] * \\beta[j]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Compressed Sparse Row (CSR) Matrix\n",
      "\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "**val:** $\\{1,2,1,1\\}$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *list of all non-zero values*  \n",
      "**col_ind:** $\\{0,3,0,1\\}$ &nbsp; *column index for each non-zero value*  \n",
      "**row_ptr:** $\\{0,1,2\\}$ &nbsp;&nbsp;&nbsp; *index into **col_ind** where each row starts*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(tweets), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (6704, 21618)\n"
       ]
      }
     ],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'first row of X\\n', X[0].nonzero()[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "first row of X\n",
        "[0]\n"
       ]
      }
     ],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'val:', X.data[:5]\n",
      "print 'col_ind:', X.indices[:5]\n",
      "print 'row_ptr:', X.indptr[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "val: [ 2.  1.  1.  1.  1.]\n",
        "col_ind: [0 1 2 3 4]\n",
        "row_ptr: [ 0  1 22 53 63]\n"
       ]
      }
     ],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'tweet 0:\\n', X[0], '\\n'\n",
      "print 'tweet 1:\\n', X[1], '\\n'\n",
      "print 'tweet 2:\\n', X[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tweet 0:\n",
        "  (0, 0)\t2.0 \n",
        "\n",
        "tweet 1:\n",
        "  (0, 1)\t1.0\n",
        "  (0, 2)\t1.0\n",
        "  (0, 3)\t1.0\n",
        "  (0, 4)\t1.0\n",
        "  (0, 5)\t1.0\n",
        "  (0, 6)\t1.0\n",
        "  (0, 7)\t1.0\n",
        "  (0, 8)\t1.0\n",
        "  (0, 9)\t1.0\n",
        "  (0, 10)\t1.0\n",
        "  (0, 11)\t1.0\n",
        "  (0, 12)\t2.0\n",
        "  (0, 13)\t1.0\n",
        "  (0, 14)\t2.0\n",
        "  (0, 15)\t1.0\n",
        "  (0, 16)\t1.0\n",
        "  (0, 17)\t1.0\n",
        "  (0, 18)\t1.0\n",
        "  (0, 19)\t1.0\n",
        "  (0, 20)\t1.0\n",
        "  (0, 21)\t1.0 \n",
        "\n",
        "tweet 2:\n",
        "  (0, 4)\t1.0\n",
        "  (0, 5)\t1.0\n",
        "  (0, 21)\t1.0\n",
        "  (0, 22)\t1.0\n",
        "  (0, 23)\t1.0\n",
        "  (0, 24)\t1.0\n",
        "  (0, 25)\t1.0\n",
        "  (0, 26)\t1.0\n",
        "  (0, 27)\t1.0\n",
        "  (0, 28)\t1.0\n",
        "  (0, 29)\t1.0\n",
        "  (0, 30)\t1.0\n",
        "  (0, 31)\t1.0\n",
        "  (0, 32)\t1.0\n",
        "  (0, 33)\t1.0\n",
        "  (0, 34)\t1.0\n",
        "  (0, 35)\t1.0\n",
        "  (0, 36)\t1.0\n",
        "  (0, 37)\t1.0\n",
        "  (0, 38)\t1.0\n",
        "  (0, 39)\t1.0\n",
        "  (0, 40)\t1.0\n",
        "  (0, 41)\t1.0\n",
        "  (0, 42)\t1.0\n",
        "  (0, 43)\t1.0\n",
        "  (0, 44)\t1.0\n",
        "  (0, 45)\t1.0\n",
        "  (0, 46)\t1.0\n",
        "  (0, 47)\t1.0\n",
        "  (0, 48)\t1.0\n",
        "  (0, 49)\t1.0\n"
       ]
      }
     ],
     "prompt_number": 235
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Efficient matrix vector product:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute z = X * \\beta, where X is a CSR matrix.\n",
      "import numpy as np\n",
      "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
      "z = np.zeros(len(tweets))\n",
      "for i in range(len(tweets)):  # for each row.\n",
      "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
      "        colidx = X.indices[j]\n",
      "        z[i] += beta[colidx] * X.data[j]\n",
      "print z[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.  23.  31.  10.  17.]\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What term has index 0 in vocabulary?\n",
      "print [term for term, idx in vocabulary.items() if idx == 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'THIS_IS_A_MENTION']\n"
       ]
      }
     ],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tweets[0]['text']\n",
      "# So, tweet 0 has 2 occurrences of the THIS_IS_A_MENTION term."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@KinkadesFC @southernmarsh\n"
       ]
      }
     ],
     "prompt_number": 238
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**4.) Create a list of gender labels.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is a 1d numpy array of gender labels.\n",
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(tweet, male_names, female_names):\n",
      "    name = get_first_name(tweet)\n",
      "    if name in female_names:\n",
      "        return 1\n",
      "    elif name in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in tweets])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: [(0, 3229), (1, 3475)]\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**5.) Fit a Logistic Regression classifier to predict gender from profile/tweet.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.693466003317\n"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fitting model with CSR much, much faster than with LIL.\n",
      "from timeit import timeit\n",
      "print 'CSR TIME'\n",
      "timeit(\"do_cross_val(X.tocsr(), y, 2)\", number=5, setup=\"from __main__ import do_cross_val, X, y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CSR TIME\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 244,
       "text": [
        "0.5761210918426514"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'CSR TIME'\n",
      "timeit(\"do_cross_val(X.tolil(), y, 2)\", number=5, setup=\"from __main__ import do_cross_val, X, y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CSR TIME\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 245,
       "text": [
        "169.74934101104736"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How does tokenization affect accuracy?\n",
      "# Collapse urls and mentions; ignore description prefix.\n",
      "def run_all(tweets, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions)\n",
      "                  for t in tweets]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    print 'acc=', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts)\n",
      "for options in option_iter:\n",
      "    print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "    run_all(tweets, *options)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "31446 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689883802464\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "35661 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691822987968\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "33137 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690330673256\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "37352 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691971907799\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "27490 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693762952575\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "31674 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695404075818\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "29182 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695852170913\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "33366 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.694509555133\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "21618 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693466003317\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "25708 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691376451078\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "23370 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695405745323\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "27463 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691824768774\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "17579 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690035504803\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "21593 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691227419947\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "19317 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690632408428\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "23334 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.692569479226\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "35701 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.692269413559\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "39930 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.697789241711\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "37392 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.694059901833\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "41621 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699280666021\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "31554 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689886584972\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "35755 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689139425913\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "33246 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690631740626\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "37447 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691675403742\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "26204 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695554553853\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "30345 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695105902256\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "27973 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.696897503534\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "32115 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695852170913\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "21862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69152592741\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "25932 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693762841275\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "23617 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695254710786\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "27688 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69450877603\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "16243 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.579803554932\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "19828 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.594419513172\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "17765 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.582040468797\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "21350 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.592630137901\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "16243 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.579803554932\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "19828 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.594419513172\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "17765 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.582040468797\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "21350 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.592630137901\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "11017 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.581295201843\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "14505 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105412535\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "12549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.580251427427\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "16044 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105189934\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "11017 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.581295201843\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "14505 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105412535\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "12549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.580251427427\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "16044 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105189934\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\n",
        "18359 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.577862922524\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\n",
        "21952 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.595613765624\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\n",
        "19881 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.578609636382\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\n",
        "23474 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.596210891849\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\n",
        "18359 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.577862922524\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\n",
        "21952 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.595613765624\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\n",
        "19881 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.578609636382\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\n",
        "23474 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.596210891849\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\n",
        "13279 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.58278562445\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\n",
        "16796 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597402250493\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\n",
        "14811 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.583382639376\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\n",
        "18335 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597104076931\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\n",
        "13279 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.58278562445\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\n",
        "16796 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597402250493\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\n",
        "14811 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.583382639376\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\n",
        "18335 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597104076931\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 214
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Error Analysis\n",
      "\n",
      "- Which ones do we get wrong?\n",
      "- Are there obvious reasons?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}