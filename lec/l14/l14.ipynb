{
 "metadata": {
  "name": "",
  "signature": "sha256:38b630296dc498b8c9b9f6de656f0a8b0d7c5c89d7202c707803f1e0f6572be6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# CS579: Lecture 14  \n",
      "\n",
      "**Demographic Inference III**\n",
      "\n",
      "*[Dr. Aron Culotta](http://cs.iit.edu/~culotta)*  \n",
      "*[Illinois Institute of Technology](http://iit.edu)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gender Classification\n",
      "\n",
      "Let's build a classifier to predict whether a Twitter user is male/female.\n",
      "\n",
      "We'll collect \"labeled\" training data using Census name list."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1.) Collect Census names. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fetch male/female names from Census.\n",
      "\n",
      "import requests\n",
      "\n",
      "def get_census_names():\n",
      "    \"\"\" Fetch a list of common male/female names from the census.\n",
      "    For ambiguous names, we select the more frequent gender.\"\"\"\n",
      "    males = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.male.first').text.split('\\n')\n",
      "    females = requests.get('http://www2.census.gov/topics/genealogy/1990surnames/dist.female.first').text.split('\\n')\n",
      "    males_pct = dict([(m.split()[0].lower(), float(m.split()[1]))\n",
      "                  for m in males if m])\n",
      "    females_pct = dict([(f.split()[0].lower(), float(f.split()[1]))\n",
      "                    for f in females if f])\n",
      "    male_names = set([m for m in males_pct if m not in females_pct or\n",
      "                  males_pct[m] > females_pct[m]])\n",
      "    female_names = set([f for f in females_pct if f not in males_pct or\n",
      "                  females_pct[f] > males_pct[f]])    \n",
      "    return male_names, female_names\n",
      "\n",
      "male_names, female_names = get_census_names()\n",
      "print 'male name sample:', list(male_names)[:5]\n",
      "print 'female name sample:', list(female_names)[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "male name sample: [u'trenton', u'darrin', u'emile', u'jason', u'ron']\n",
        "female name sample: [u'fawn', u'kymberly', u'augustina', u'evalyn', u'chieko']\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2.) Sample 5K tweets with names on the Census list. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Construct TwitterAPI object.\n",
      "\n",
      "import ConfigParser\n",
      "from TwitterAPI import TwitterAPI\n",
      "\n",
      "def get_twitter(config_file):\n",
      "    config = ConfigParser.ConfigParser()\n",
      "    config.read(config_file)\n",
      "    twitter = TwitterAPI(\n",
      "                   config.get('twitter', 'consumer_key'),\n",
      "                   config.get('twitter', 'consumer_secret'),\n",
      "                   config.get('twitter', 'access_token'),\n",
      "                   config.get('twitter', 'access_token_secret'))\n",
      "    return twitter\n",
      "\n",
      "twitter = get_twitter('twitter.cfg')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_first_name(tweet):\n",
      "    if 'user' in tweet and 'name' in tweet['user']:\n",
      "        parts = tweet['user']['name'].split()\n",
      "        if len(parts) > 0:\n",
      "            return parts[0].lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Sample U.S. tweets with names from Census. \n",
      "import sys\n",
      "\n",
      "\n",
      "def sample_tweets(twitter, limit, male_names, female_names):\n",
      "    tweets = []\n",
      "    while True:\n",
      "        try:\n",
      "            # Restrict to U.S.\n",
      "            for response in twitter.request('statuses/filter',\n",
      "                        {'locations':'-124.637,24.548,-66.993,48.9974'}):\n",
      "                if 'user' in response:\n",
      "                    name = get_first_name(response)\n",
      "                    if name in male_names or name in female_names:\n",
      "                        tweets.append(response)\n",
      "                        if len(tweets) % 100 == 0:\n",
      "                            print 'found %d tweets' % len(tweets)\n",
      "                        if len(tweets) >= limit:\n",
      "                            return tweets\n",
      "        except:\n",
      "            print \"Unexpected error:\", sys.exc_info()[0]\n",
      "    return tweets\n",
      "        \n",
      "tweets = sample_tweets(twitter, 5000, male_names, female_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "found 100 tweets\n",
        "found 200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 1900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 2900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unexpected error:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <class 'requests.exceptions.ChunkedEncodingError'>\n",
        "found 3300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 3900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 4900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5800 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 5900 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6000 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6100 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6200 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6300 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6400 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6500 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6600 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "found 6700 tweets"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unexpected error:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " <class 'requests.exceptions.ChunkedEncodingError'>\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "print 'sampled %d tweets' % len(tweets)\n",
      "print 'top names:', Counter(get_first_name(t) for t in tweets).most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " sampled 6704 tweets\n",
        "top names: [(u'michael', 62), (u'nick', 55), (u'jay', 52), (u'justin', 51), (u'amanda', 50), (u'mike', 50), (u'john', 49), (u'chris', 48), (u'ryan', 47), (u'steve', 46)]\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Save these tweets.\n",
      "import pickle\n",
      "pickle.dump(tweets, open('tweets.pkl', 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load these tweets\n",
      "import pickle\n",
      "tweets = pickle.load(open('tweets.pkl', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3.) Tokenize tweets. **"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tweet = tweets[7]\n",
      "print 'test tweet:\\n\\tdescr=%s\\n\\ttext=%s' % (test_tweet['user']['description'],\n",
      "                                              test_tweet['text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "test tweet:\n",
        "\tdescr=Supercomplicated, goofy, Catholic, future tv personality, Hokie, runner, musician http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ\n",
        "\ttext=@JakesPartner thanks prayer partner. I really needed that because I've been having a really rough week.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "def tokenize(string, lowercase, keep_punctuation, prefix,\n",
      "             collapse_urls, collapse_mentions):\n",
      "    if not string:\n",
      "        return []\n",
      "    if lowercase:\n",
      "        string = string.lower()\n",
      "    tokens = []\n",
      "    if collapse_urls:\n",
      "        string = re.sub('http\\S+', 'THIS_IS_A_URL', string)\n",
      "    if collapse_mentions:\n",
      "        string = re.sub('@\\S+', 'THIS_IS_A_MENTION', string)\n",
      "    if keep_punctuation:\n",
      "        tokens = string.split()\n",
      "    else:\n",
      "        tokens = re.sub('\\W+', ' ', string).split()\n",
      "    if prefix:\n",
      "        tokens = ['%s%s' % (prefix, t) for t in tokens]\n",
      "    return tokens\n",
      "\n",
      "def tweet2tokens(tweet, use_descr=True, lowercase=True,\n",
      "                 keep_punctuation=True, descr_prefix='d=',\n",
      "                 collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    tokens = []\n",
      "    if use_text:\n",
      "        tokens.extend(tokenize(tweet['text'], lowercase, keep_punctuation, None,\n",
      "                           collapse_urls, collapse_mentions))\n",
      "    if use_descr:\n",
      "        tokens.extend(tokenize(tweet['user']['description'], lowercase,\n",
      "                               keep_punctuation, descr_prefix,\n",
      "                               collapse_urls, collapse_mentions))\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for enumerating all possible arguments of tweet2tokens\n",
      "# https://docs.python.org/2/library/itertools.html#itertools.product\n",
      "from itertools import product\n",
      "\n",
      "use_descr_opts = [True, False]\n",
      "lowercase_opts = [True, False]\n",
      "keep_punctuation_opts = [True, False]\n",
      "descr_prefix_opts = ['d=', '']\n",
      "url_opts = [True, False]\n",
      "mention_opts = [True, False]\n",
      "use_text_opts = [True, False]\n",
      "\n",
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts, use_text_opts)\n",
      "for options in option_iter:\n",
      "    print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "    print\n",
      "    print '    '.join(tweet2tokens(test_tweet, *options)), '\\n----\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "d=supercomplicated,    d=goofy,    d=catholic,    d=future    d=tv    d=personality,    d=hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week.    supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "supercomplicated,    goofy,    catholic,    future    tv    personality,    hokie,    runner,    musician    http://www.youtube.com/channel/ucboqgx_pax-rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "d=supercomplicated    d=goofy    d=catholic    d=future    d=tv    d=personality    d=hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=ucboqgx_pax    d=rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week    supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "supercomplicated    goofy    catholic    future    tv    personality    hokie    runner    musician    http    www    youtube    com    channel    ucboqgx_pax    rq9nvdpqxulq \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "d=Supercomplicated,    d=goofy,    d=Catholic,    d=future    d=tv    d=personality,    d=Hokie,    d=runner,    d=musician    d=http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week.    Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "Supercomplicated,    goofy,    Catholic,    future    tv    personality,    Hokie,    runner,    musician    http://www.youtube.com/channel/UCboqgX_Pax-rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "d=Supercomplicated    d=goofy    d=Catholic    d=future    d=tv    d=personality    d=Hokie    d=runner    d=musician    d=http    d=www    d=youtube    d=com    d=channel    d=UCboqgX_Pax    d=rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        "Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        "Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    THIS_IS_A_URL \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        "Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week    Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        "Supercomplicated    goofy    Catholic    future    tv    personality    Hokie    runner    musician    http    www    youtube    com    channel    UCboqgX_Pax    rq9NVDPqXUlQ \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@jakespartner    thanks    prayer    partner.    i    really    needed    that    because    i've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "jakespartner    thanks    prayer    partner    i    really    needed    that    because    i    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "@JakesPartner    thanks    prayer    partner.    I    really    needed    that    because    I've    been    having    a    really    rough    week. \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "\n",
        "THIS_IS_A_MENTION    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "\n",
        "JakesPartner    thanks    prayer    partner    I    really    needed    that    because    I    ve    been    having    a    really    rough    week \n",
        "----\n",
        "\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "\n",
        " \n",
        "----\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's tokenize all tweets.\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=False, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in tweets]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokens_list[7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'THIS_IS_A_MENTION', u'thanks', u'prayer', u'partner', u'i', u'really', u'needed', u'that', u'because', u'i', u've', u'been', u'having', u'a', u'really', u'rough', u'week', u'd=supercomplicated', u'd=goofy', u'd=catholic', u'd=future', u'd=tv', u'd=personality', u'd=hokie', u'd=runner', u'd=musician', u'd=THIS_IS_A_URL']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Store these in a sparse matrix.\n",
      "\n",
      "#1) Create a vocabulary (dict from term->index)\n",
      "\n",
      "# https://docs.python.org/2/library/collections.html#collections.defaultdict\n",
      "from collections import defaultdict\n",
      "\n",
      "def make_vocabulary(tokens_list):\n",
      "    vocabulary = defaultdict(lambda: len(vocabulary))\n",
      "    for tokens in tokens_list:\n",
      "        for token in tokens:\n",
      "            vocabulary[token]\n",
      "    print '%d unique terms in vocabulary' % len(vocabulary)\n",
      "    return vocabulary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21618 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31446 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation and urls?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=False, collapse_mentions=True)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33137 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How big is vocabulary if we keep punctuation and urls and mentions?\n",
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=False, collapse_mentions=False)\n",
      "              for t in tweets]\n",
      "\n",
      "vocabulary = make_vocabulary(tokens_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "37352 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Feature Vector Matrix\n",
      "\n",
      "Create a matrix $X$ where $X[i,j]$ is the frequenct of term $j$ in tweet $i$.\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sparse Matrices\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "$X$ is mostly $0$ for text problems."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## List of List (LIL) Matrix\n",
      "\n",
      "Store a linked list of (index, value) pairs for each row.\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "\\hbox{tweet}_1 & (0, 1)\\\\\n",
      "\\hbox{tweet}_2 & (3,2)\\\\\n",
      "\\hbox{tweet}_3 & (0,1), (1,1)\\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "**Advantage:** Fast to construct: append to list in constant time.\n",
      "\n",
      "**Disadvantage:** Slow random access for matrix-vector product.\n",
      "\n",
      "E.g., $\\hat{z} = X\\cdot \\hat{\\beta}$ to classify tweets using a learned weight vector $\\beta$\n",
      "\n",
      "$\\hat{z}[i] = \\sum_j X[i,j] * \\beta[j]$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Compressed Sparse Row (CSR) Matrix\n",
      "\n",
      "\n",
      "$$\n",
      "X = \\begin{pmatrix}\n",
      "~ & \\hbox{term}_1 & \\hbox{term}_2 & \\hbox{term}_3 & \\hbox{term}_4 \\\\\n",
      "\\hbox{tweet}_1 & 1  &  0  &  0 & 0 \\\\\n",
      "\\hbox{tweet}_2 & 0  &  0  &  0 & 2 \\\\\n",
      "\\hbox{tweet}_3 & 1  &  1  &  0 & 0 \\\\\n",
      "\\end{pmatrix}\n",
      "$$\n",
      "\n",
      "**val:** $\\{1,2,1,1\\}$  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *list of all non-zero values*  \n",
      "**col_ind:** $\\{0,3,0,1\\}$ &nbsp; *column index for each non-zero value*  \n",
      "**row_ptr:** $\\{0,1,2\\}$ &nbsp;&nbsp;&nbsp; *index into **col_ind** where each row starts*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Convert features to a sparse matrix X.\n",
      "# X[i,j] is the frequency of term j in tweet i\n",
      "# \n",
      "from scipy.sparse import lil_matrix\n",
      "\n",
      "def make_feature_matrix(tokens_list, vocabulary):\n",
      "    X = lil_matrix((len(tweets), len(vocabulary)))\n",
      "    for i, tokens in enumerate(tokens_list):\n",
      "        for token in tokens:\n",
      "            j = vocabulary[token]\n",
      "            X[i,j] += 1\n",
      "    return X.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = make_feature_matrix(tokens_list, vocabulary)\n",
      "print 'shape of X:', X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "shape of X: (6704, 37352)\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'first row of X\\n', X[0].nonzero()[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "first row of X\n",
        "[0 1]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'val:', X.data[:5]\n",
      "print 'col_ind:', X.indices[:5]\n",
      "print 'row_ptr:', X.indptr[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "val: [ 1.  1.  1.  1.  1.]\n",
        "col_ind: [0 1 2 3 4]\n",
        "row_ptr: [ 0  2 23 53 63]\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'tweet 0:\\n', X[0], '\\n'\n",
      "print 'tweet 1:\\n', X[1], '\\n'\n",
      "print 'tweet 2:\\n', X[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "tweet 0:\n",
        "  (0, 0)\t1.0\n",
        "  (0, 1)\t1.0 \n",
        "\n",
        "tweet 1:\n",
        "  (0, 2)\t1.0\n",
        "  (0, 3)\t1.0\n",
        "  (0, 4)\t1.0\n",
        "  (0, 5)\t1.0\n",
        "  (0, 6)\t1.0\n",
        "  (0, 7)\t1.0\n",
        "  (0, 8)\t1.0\n",
        "  (0, 9)\t1.0\n",
        "  (0, 10)\t1.0\n",
        "  (0, 11)\t1.0\n",
        "  (0, 12)\t1.0\n",
        "  (0, 13)\t2.0\n",
        "  (0, 14)\t1.0\n",
        "  (0, 15)\t2.0\n",
        "  (0, 16)\t1.0\n",
        "  (0, 17)\t1.0\n",
        "  (0, 18)\t1.0\n",
        "  (0, 19)\t1.0\n",
        "  (0, 20)\t1.0\n",
        "  (0, 21)\t1.0\n",
        "  (0, 22)\t1.0 \n",
        "\n",
        "tweet 2:\n",
        "  (0, 5)\t1.0\n",
        "  (0, 23)\t1.0\n",
        "  (0, 24)\t1.0\n",
        "  (0, 25)\t1.0\n",
        "  (0, 26)\t1.0\n",
        "  (0, 27)\t1.0\n",
        "  (0, 28)\t1.0\n",
        "  (0, 29)\t1.0\n",
        "  (0, 30)\t1.0\n",
        "  (0, 31)\t1.0\n",
        "  (0, 32)\t1.0\n",
        "  (0, 33)\t1.0\n",
        "  (0, 34)\t1.0\n",
        "  (0, 35)\t5.0\n",
        "  (0, 36)\t1.0\n",
        "  (0, 37)\t1.0\n",
        "  (0, 38)\t1.0\n",
        "  (0, 39)\t1.0\n",
        "  (0, 40)\t1.0\n",
        "  (0, 41)\t1.0\n",
        "  (0, 42)\t1.0\n",
        "  (0, 43)\t1.0\n",
        "  (0, 44)\t1.0\n",
        "  (0, 45)\t2.0\n",
        "  (0, 46)\t1.0\n",
        "  (0, 47)\t1.0\n",
        "  (0, 48)\t1.0\n",
        "  (0, 49)\t1.0\n",
        "  (0, 50)\t1.0\n",
        "  (0, 51)\t1.0\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Efficient matrix vector product:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute z = X * \\beta, where X is a CSR matrix.\n",
      "import numpy as np\n",
      "beta = np.ones(len(vocabulary))  # assume Beta = vector of 1s\n",
      "z = np.zeros(len(tweets))\n",
      "for i in range(len(tweets)):  # for each row.\n",
      "    for j in range(X.indptr[i], X.indptr[i+1]): # for each col.\n",
      "        colidx = X.indices[j]\n",
      "        z[i] += beta[colidx] * X.data[j]\n",
      "print z[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.  23.  35.  10.  13.]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What term has index 0 in vocabulary?\n",
      "print [term for term, idx in vocabulary.items() if idx == 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'@kinkadesfc']\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tweets[0]['text']\n",
      "# So, tweet 0 has 2 terms."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "@KinkadesFC @southernmarsh\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**4.) Create a list of gender labels.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# y is a 1d numpy array of gender labels.\n",
      "# Let 1=Female, 0=Male.\n",
      "import numpy as np\n",
      "\n",
      "def get_gender(tweet, male_names, female_names):\n",
      "    name = get_first_name(tweet)\n",
      "    if name in female_names:\n",
      "        return 1\n",
      "    elif name in male_names:\n",
      "        return 0\n",
      "    else:\n",
      "        return -1\n",
      "    \n",
      "y = np.array([get_gender(t, male_names, female_names) for t in tweets])\n",
      "print 'gender labels:', Counter(y).items()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gender labels: [(0, 3229), (1, 3475)]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**5.) Fit a Logistic Regression classifier to predict gender from profile/tweet.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Do 5-fold cross-validation\n",
      "# http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix\n",
      "\n",
      "def do_cross_val(X, y, nfolds):\n",
      "    \"\"\" Compute average cross-validation acccuracy.\"\"\"\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    accuracies = []\n",
      "    for train_idx, test_idx in cv:\n",
      "        clf = LogisticRegression()\n",
      "        clf.fit(X[train_idx], y[train_idx])\n",
      "        predicted = clf.predict(X[test_idx])\n",
      "        acc = accuracy_score(y[test_idx], predicted)\n",
      "        accuracies.append(acc)\n",
      "    avg = np.mean(accuracies)\n",
      "    return avg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'avg accuracy', do_cross_val(X, y, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "avg accuracy "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.691971907799\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fitting model with CSR much, much faster than with LIL.\n",
      "from timeit import timeit\n",
      "print 'CSR TIME'\n",
      "timeit(\"do_cross_val(X.tocsr(), y, 2)\", number=5, setup=\"from __main__ import do_cross_val, X, y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CSR TIME\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "0.5333049297332764"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'CSR TIME'\n",
      "timeit(\"do_cross_val(X.tolil(), y, 2)\", number=5, setup=\"from __main__ import do_cross_val, X, y\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CSR TIME\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 245,
       "text": [
        "169.74934101104736"
       ]
      }
     ],
     "prompt_number": 245
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# How does tokenization affect accuracy?\n",
      "# Collapse urls and mentions; ignore description prefix.\n",
      "def run_all(tweets, use_descr=True, lowercase=True,\n",
      "            keep_punctuation=True, descr_prefix=None,\n",
      "            collapse_urls=True, collapse_mentions=True, use_text=True):\n",
      "    \n",
      "    tokens_list = [tweet2tokens(t, use_descr, lowercase,\n",
      "                            keep_punctuation, descr_prefix,\n",
      "                            collapse_urls, collapse_mentions, use_text)\n",
      "                  for t in tweets]\n",
      "    vocabulary = make_vocabulary(tokens_list)\n",
      "    X = make_feature_matrix(tokens_list, vocabulary)\n",
      "    acc = do_cross_val(X, y, 5)\n",
      "    print 'acc=', acc\n",
      "    return acc\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "run_all(tweets, use_descr=True, use_text=True)\n",
      "run_all(tweets, use_descr=True, use_text=False)\n",
      "run_all(tweets, use_descr=False, use_text=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "27490 unique terms in vocabulary\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693762952575\n",
        "15203 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699130187986\n",
        "16243 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.579803554932\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "argnames = ['use_descr', 'lower', 'punct', 'prefix', 'url', 'mention', 'use_text']\n",
      "option_iter = product(use_descr_opts, lowercase_opts,\n",
      "                       keep_punctuation_opts,\n",
      "                       descr_prefix_opts, url_opts,\n",
      "                       mention_opts, use_text_opts)\n",
      "\n",
      "results = []\n",
      "for options in option_iter:\n",
      "    if options[0] or options[-1]:  # skip options that don't use descr or text.\n",
      "        print '\\t'.join('%s=%s' % (name, opt) for name, opt in zip(argnames, options))\n",
      "        acc = run_all(tweets, *options)\n",
      "        results.append((options, acc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "31446 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689883802464\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "15203 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699130187986\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "35661 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691822987968\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "15833 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69972675771\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "33137 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690330673256\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "15372 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.702262512939\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "37352 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691971907799\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "16002 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.701814529144\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "27490 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693762952575\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "15203 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699130187986\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "31674 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695404075818\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "15833 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69972675771\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "29182 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695852170913\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "15372 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.702262512939\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "33366 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.694509555133\n",
        "use_descr=True\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "16002 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.701814529144\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "21618 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693466003317\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "10601 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699579841286\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "25708 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691376451078\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "11203 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700027379879\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "23370 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695405745323\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "10821 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699430698855\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "27463 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691824768774\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "11419 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700624060904\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "17579 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690035504803\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "10601 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699579841286\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "21593 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691227419947\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "11203 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700027379879\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "19317 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690632408428\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "10821 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699430698855\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "23334 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.692569479226\n",
        "use_descr=True\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "11419 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700624060904\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "35701 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.692269413559\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "17342 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699877235745\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "39930 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.697789241711\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "17978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700026600777\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "37392 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.694059901833\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "17511 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69913152359\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "41621 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699280666021\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "18147 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699877458346\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "31554 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689886584972\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "17342 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699877235745\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "35755 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.689139425913\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "17978 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700026600777\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "33246 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.690631740626\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "17511 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69913152359\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "37447 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.691675403742\n",
        "use_descr=True\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "18147 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.699877458346\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "26204 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695554553853\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=False\n",
        "12925 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.698835130834\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "30345 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695105902256\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=False\n",
        "13549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.698238227208\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "27973 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.696897503534\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=False\n",
        "13162 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700625730408\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "32115 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695852170913\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=False\n",
        "13780 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700625285207\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "21862 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69152592741\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=False\n",
        "12925 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.698835130834\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "25932 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.693762841275\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=False\n",
        "13549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.698238227208\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "23617 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.695254710786\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=False\n",
        "13162 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700625730408\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "27688 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.69450877603\n",
        "use_descr=True\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=False\n",
        "13780 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.700625285207\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "16243 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.579803554932\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "19828 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.594419513172\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "17765 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.582040468797\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "21350 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.592630137901\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "16243 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.579803554932\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "19828 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.594419513172\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "17765 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.582040468797\n",
        "use_descr=False\tlower=True\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "21350 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.592630137901\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "11017 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.581295201843\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "14505 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105412535\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "12549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.580251427427\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "16044 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105189934\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "11017 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.581295201843\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "14505 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105412535\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "12549 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.580251427427\n",
        "use_descr=False\tlower=True\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "16044 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597105189934\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "18359 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.577862922524\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "21952 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.595613765624\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "19881 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.578609636382\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "23474 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.596210891849\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "18359 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.577862922524\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "21952 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.595613765624\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "19881 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.578609636382\n",
        "use_descr=False\tlower=False\tpunct=True\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "23474 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.596210891849\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=True\tuse_text=True\n",
        "13279 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.58278562445\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=True\tmention=False\tuse_text=True\n",
        "16796 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597402250493\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=True\tuse_text=True\n",
        "14811 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.583382639376\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=d=\turl=False\tmention=False\tuse_text=True\n",
        "18335 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597104076931\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=True\tuse_text=True\n",
        "13279 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.58278562445\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=True\tmention=False\tuse_text=True\n",
        "16796 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597402250493\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=True\tuse_text=True\n",
        "14811 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.583382639376\n",
        "use_descr=False\tlower=False\tpunct=False\tprefix=\turl=False\tmention=False\tuse_text=True\n",
        "18335 unique terms in vocabulary"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "acc="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.597104076931\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What's the best setting?\n",
      "best_result = max(results, key=lambda x: x[1])\n",
      "print 'best result:\\n\\tacc=%g\\n\\toptions=%s' % (best_result[1],\n",
      "                                                zip(argnames, best_result[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "best result:\n",
        "\tacc=0.702263\n",
        "\toptions=[('use_descr', True), ('lower', True), ('punct', True), ('prefix', 'd='), ('url', False), ('mention', True), ('use_text', False)]\n"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Which decisions had the biggest effect?\n",
      "# Find max score with option=True and with option=False\n",
      "\n",
      "def find_max(results, i, value):\n",
      "    return max(accuracy for options, accuracy in results if options[i] == value)\n",
      "\n",
      "best_scores = []\n",
      "for i, argname in enumerate(argnames):\n",
      "    values = [True, False] if argname != 'prefix' else ['d=', ''] # hack to deal with prefix option.\n",
      "    best_for_true = find_max(results, i, values[0])\n",
      "    best_for_false = find_max(results, i, values[1]) \n",
      "    print argname, best_for_true, best_for_false\n",
      "    best_scores.append((argname, best_for_true, best_for_false))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "use_descr 0.702262512939 0.597402250493\n",
        "lower 0.702262512939 0.700625730408\n",
        "punct 0.702262512939 0.700625730408\n",
        "prefix 0.702262512939 0.702262512939\n",
        "url 0.700027379879 0.702262512939\n",
        "mention 0.702262512939 0.701814529144\n",
        "use_text 0.699280666021 0.702262512939\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot results as stacked bar chart.\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "srted = np.argsort([min(b[1], b[2]) for b in best_scores])\n",
      "best_scores = np.array(best_scores)[srted]\n",
      "print best_scores\n",
      "ind = np.arange(len(best_scores))\n",
      "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
      "p1 = plt.bar(ind, [float(s[1]) for s in best_scores], width, color='r')\n",
      "p2 = plt.bar(ind, [float(s[2]) for s in best_scores], width, color='y')\n",
      "plt.ylabel('accuracy')\n",
      "plt.title('Accuracy by option')\n",
      "plt.xticks(ind+width/2., np.array(argnames)[srted] )\n",
      "plt.ylim((.58, .72))\n",
      "plt.legend((p1[0], p2[0]), ('True', 'False'), loc='center left', bbox_to_anchor=(1, 0.5))\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['use_descr' '0.702262512939' '0.597402250493']\n",
        " ['use_text' '0.699280666021' '0.702262512939']\n",
        " ['url' '0.700027379879' '0.702262512939']\n",
        " ['lower' '0.702262512939' '0.700625730408']\n",
        " ['punct' '0.702262512939' '0.700625730408']\n",
        " ['mention' '0.702262512939' '0.701814529144']\n",
        " ['prefix' '0.702262512939' '0.702262512939']]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAdMAAAELCAYAAACCk2zvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNoBZ9c4AAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10ebc20d0>"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Error Analysis\n",
      "\n",
      "- What terms does the model weight highly?\n",
      "- Which examples do we get wrong?\n",
      "- Are there obvious reasons?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens_list = [tweet2tokens(t, use_descr=True, lowercase=True,\n",
      "                            keep_punctuation=True, descr_prefix='d=',\n",
      "                            collapse_urls=True, collapse_mentions=True, use_text=True)\n",
      "                for t in tweets]\n",
      "vocabulary = make_vocabulary(tokens_list)\n",
      "X = make_feature_matrix(tokens_list, vocabulary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31446 unique terms in vocabulary\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_terms(vocab):\n",
      "    return np.array([x[0] for x in sorted(vocab.items(), key=lambda x: x[1])])\n",
      "    \n",
      "def print_top_terms(model, terms, n=20):\n",
      "    print '\\nTop Coefficients'\n",
      "    coef = model.coef_[0]\n",
      "    srted = np.argsort(coef)\n",
      "    topi = srted[::-1][:n]\n",
      "    boti = srted[:n]\n",
      "    print 'Female Terms:\\n' + '\\n'.join('%s (%g)' % (n, c) for n, c in zip(terms[topi], coef[topi]))\n",
      "    print '\\nMale Terms:\\n' + '\\n'.join('%s (%g)' % (n, c) for n, c in zip(terms[boti], coef[boti]))\n",
      "    print '\\nintercept=%g' % model.intercept_\n",
      "\n",
      "def print_terms_and_coef(row, terms, coef):\n",
      "    indices = sorted(row.indices, key=lambda x: coef[x])\n",
      "    print 'Top Terms:'\n",
      "    for i in indices:\n",
      "        if coef[i] != 0:\n",
      "            print terms[i], coef[i]\n",
      "    print\n",
      "    \n",
      "def error_analysis(X, tokens_list, tweets, vocabulary):\n",
      "    \"\"\" Print tweets that we got wrong. \"\"\"\n",
      "    terms = get_terms(vocabulary)\n",
      "    clf = LogisticRegression()\n",
      "    train_idx = np.arange(4000)\n",
      "    test_idx = np.arange(4000, len(tokens_list))\n",
      "    X_train, y_train = X[train_idx], y[train_idx]\n",
      "    X_test, y_test = X[test_idx], y[test_idx]\n",
      "    clf.fit(X_train, y_train)\n",
      "    predicted = clf.predict(X_test)\n",
      "    print 'accuracy=%g' % accuracy_score(y[test_idx], predicted)\n",
      "    print_top_terms(clf, terms)\n",
      "    predicted_proba = clf.predict_proba(X_test)\n",
      "    print '\\nERRORS:'\n",
      "    for i in range(len(predicted)):\n",
      "        probability = predicted_proba[i][predicted[i]]\n",
      "        # If we're very wrong.\n",
      "        if predicted[i] != y_test[i] and probability > .95:\n",
      "            og_idx = test_idx[i]\n",
      "            print '\\npred=%d (%g) truth=%d \\ntext=%s \\ndescr=%s \\nname=%s' % (predicted[i],\n",
      "                                                                  probability,\n",
      "                                                                  y_test[i],\n",
      "                                                                  tweets[og_idx]['text'],\n",
      "                                                                  tweets[og_idx]['user']['description'],\n",
      "                                                                  tweets[og_idx]['user']['name'])\n",
      "            print_terms_and_coef(X.getrow(og_idx), terms, clf.coef_[0])\n",
      "    \n",
      "error_analysis(X, tokens_list, tweets, vocabulary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accuracy=0.660133\n",
        "\n",
        "Top Coefficients\n",
        "Female Terms:\n",
        "d=girl (1.47977)\n",
        "d=happy (1.13249)\n",
        "d=lover (1.10084)\n",
        "d=wife, (1.07527)\n",
        "d=\u2661 (1.04744)\n",
        "hair (1.04582)\n",
        "d=lady (1.03934)\n",
        "actually (1.01849)\n",
        "\ud83d\ude29 (1.01611)\n",
        "d=love. (0.996009)\n",
        "d=:) (0.995295)\n",
        "making (0.982886)\n",
        "d=pr (0.981714)\n",
        "finally (0.956199)\n",
        "d=\u270c\ufe0f (0.953087)\n",
        "myself (0.949623)\n",
        "d=\u2600\ufe0f (0.94103)\n",
        "down (0.90597)\n",
        "please (0.900559)\n",
        "taking (0.890297)\n",
        "\n",
        "Male Terms:\n",
        "d=baseball (-1.77623)\n",
        "d=guy (-1.45502)\n",
        "d=time (-1.3902)\n",
        "d=livin (-1.33779)\n",
        "d=husband (-1.21616)\n",
        "help (-1.17115)\n",
        "d=short. (-1.13)\n",
        "d=father (-1.12302)\n",
        "d=still (-1.05689)\n",
        "show (-1.0389)\n",
        "rko (-1.0308)\n",
        "d=sports (-0.976833)\n",
        "d=very (-0.961421)\n",
        "d=tweet (-0.941346)\n",
        "d=2015. (-0.905699)\n",
        "d=father, (-0.903277)\n",
        "d=always (-0.897511)\n",
        "d=husband, (-0.889091)\n",
        "dogs (-0.887246)\n",
        "wish (-0.8847)\n",
        "\n",
        "intercept=0.226097\n",
        "\n",
        "ERRORS:\n",
        "\n",
        "pred=1 (0.984152) truth=0 \n",
        "text=I really want fried chicken, mashed potatoes, yellow rice, and macaroni. #fatstatus \ud83d\udc4c\ud83d\ude48 \n",
        "descr=Be careful what you think about; your thoughts run your life. \n",
        "name=Taylor Nowling\u270c\ufe0f\n",
        "Top Terms:\n",
        "d=thoughts -0.342287237334\n",
        "yellow -0.129052460857\n",
        "d=run -0.0997258853366\n",
        "and 0.0260745372736\n",
        "d=you 0.0343911157781\n",
        "d=careful 0.102485700479\n",
        "want 0.170624627072\n",
        "i 0.264195051906\n",
        "d=what 0.310553213443\n",
        "d=think 0.523837627761\n",
        "d=life. 0.53202545457\n",
        "d=your 0.541063195391\n",
        "really 0.562122715075\n",
        "d=be 0.865270812687\n",
        "\n",
        "\n",
        "pred=1 (0.969715) truth=0 \n",
        "text=Otra derrota y esta ves la culpa fue del sartorio.... http://t.co/oNREqZLgAj \n",
        "descr=Married with children/me rio solo y no estoy loco/invento chistes trato de hacer deportes mgg21@hotmail.com \n",
        "name=Gary Gonzalez\n",
        "Top Terms:\n",
        "d=de -0.133163722803\n",
        "ves -0.0674602626871\n",
        "esta -0.0603836720679\n",
        "d=deportes -0.0509922349532\n",
        "THIS_IS_A_URL 0.0173468434062\n",
        "del 0.0367209738787\n",
        "d=rio 0.0645514102377\n",
        "fue 0.182248576827\n",
        "d=with 0.22957274737\n",
        "d=solo 0.269066710597\n",
        "d=y 0.287760645288\n",
        "otra 0.319704262741\n",
        "la 0.375333789862\n",
        "d=married 0.387633200111\n",
        "d=estoy 0.417966006046\n",
        "d=no 0.475961874721\n",
        "y 0.488368153253\n",
        "\n",
        "\n",
        "pred=0 (0.957932) truth=1 \n",
        "text=@seriouseats if using canned, you still need to take the skins off \n",
        "descr=retired nurse, working on healthy cookbook, disease oriented for diabetics, kidney disease, dementia, etc \n",
        "name=karin marraccini\n",
        "Top Terms:\n",
        "take -0.558749392908\n",
        "d=retired -0.545942323091\n",
        "off -0.4906517055\n",
        "if -0.361650338067\n",
        "d=for -0.332316336481\n",
        "still -0.296685732032\n",
        "using -0.287678662477\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        "d=working -0.126431644331\n",
        "d=nurse, -0.114527055602\n",
        "d=on -0.103115415443\n",
        "the -0.0534844656456\n",
        "need -0.0520172216396\n",
        "to -0.0291718973355\n",
        "d=healthy -0.0037685004917\n",
        "you 0.0903342624185\n",
        "d=etc 0.122844182693\n",
        "\n",
        "\n",
        "pred=1 (0.961612) truth=0 \n",
        "text=@AmandaBoo_Xo @toddcarey put yourself in my shoes what would you do if he comented the person below you then skipped over u to the next... \n",
        "descr=Photographer, I \u2764\ufe0f Todd Carey and Leo Cody! \u270c\ufe0fCali Chick \u2600\ufe0f 4/27/14 \u2764\ufe0f \n",
        "name=Sam Vaught\n",
        "Top Terms:\n",
        "if -0.361650338067\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        "d=and -0.156067253542\n",
        "then -0.134829415183\n",
        "in -0.121804372095\n",
        "would -0.0732397618555\n",
        "u -0.0583817191763\n",
        "the -0.0534844656456\n",
        "yourself -0.0418556363813\n",
        "d=photographer, -0.03786040257\n",
        "to -0.0291718973355\n",
        "do 0.000574156846851\n",
        "d=carey 0.0333239808445\n",
        "over 0.0565240262375\n",
        "he 0.0586369117433\n",
        "d=i 0.0865988954007\n",
        "you 0.0903342624185\n",
        "what 0.172881360463\n",
        "d=chick 0.182499149937\n",
        "my 0.183124635701\n",
        "put 0.229970752274\n",
        "person 0.483315496845\n",
        "shoes 0.505049260642\n",
        "d=\u2764\ufe0f 0.709777584933\n",
        "d=\u2600\ufe0f 0.941029506796\n",
        "\n",
        "\n",
        "pred=1 (0.963435) truth=0 \n",
        "text=What to make myself for dinner tonight...decisions, decisions. http://t.co/pkkfcDQtDZ \n",
        "descr=We are important, our lives are important, and our details are worthy of being recorded. Life on Earth is really kind of amazing. \n",
        "name=Galen Reese\n",
        "Top Terms:\n",
        "d=really -0.552414937473\n",
        "d=of -0.278956949134\n",
        "d=and -0.156067253542\n",
        "d=kind -0.108500319053\n",
        "d=on -0.103115415443\n",
        "d=earth -0.0716532853239\n",
        "d=being -0.0582340760655\n",
        "for -0.0441533201718\n",
        "to -0.0291718973355\n",
        "make -0.00430052956028\n",
        "THIS_IS_A_URL 0.0173468434062\n",
        "d=amazing. 0.124601681916\n",
        "d=is 0.127503382315\n",
        "what 0.172881360463\n",
        "d=life 0.209038170022\n",
        "d=are 0.234630342547\n",
        "dinner 0.237776170322\n",
        "d=we 0.574461431799\n",
        "d=our 0.80685965797\n",
        "myself 0.949622843835\n",
        "\n",
        "\n",
        "pred=1 (0.95698) truth=0 \n",
        "text=\"You're way too beautiful girl\n",
        "That's why it'll never work\n",
        "You'll have me suicidal, suicidal\n",
        "When you say it's over\" \n",
        "descr=Bringing suck it back. Junior at Batavia High School 17. EMILY LAUREN ROTH \u2764\ufe0f is baby girl 6/4/2014. @SicklesJoe and josh stephens are my brothers. \n",
        "name=Kyle Seipel\u2122\n",
        "Top Terms:\n",
        "d=at -0.749191060088\n",
        "that's -0.648247954488\n",
        "d=17. -0.371091773867\n",
        "d=josh -0.294961697052\n",
        "girl -0.256143776112\n",
        "d=school -0.159377205395\n",
        "d=and -0.156067253542\n",
        "you'll -0.152821131054\n",
        "d=junior -0.147207452566\n",
        "d=THIS_IS_A_MENTION -0.129819685657\n",
        "never -0.126959763931\n",
        "d=high -0.114650225059\n",
        "d=it -0.079165795731\n",
        "it'll -0.0658551273251\n",
        "d=bringing -0.0403833729313\n",
        "d=back. -0.0331911632993\n",
        "d=lauren 0.0194935225806\n",
        "way 0.0217647399252\n",
        "why 0.0425014495999\n",
        "d=my 0.0858944931126\n",
        "you 0.0903342624185\n",
        "\"you're 0.101463193699\n",
        "d=is 0.127503382315\n",
        "beautiful 0.172195475632\n",
        "d=suck 0.192022876771\n",
        "have 0.213284201436\n",
        "work 0.227076472782\n",
        "d=are 0.234630342547\n",
        "it's 0.337116157443\n",
        "me 0.359690531198\n",
        "say 0.404922569887\n",
        "d=baby 0.427215414359\n",
        "too 0.499943153552\n",
        "when 0.654548137388\n",
        "d=\u2764\ufe0f 0.709777584933\n",
        "d=girl 1.4797733554\n",
        "\n",
        "\n",
        "pred=1 (0.981796) truth=0 \n",
        "text=\"@OMGITSMIB: It's only fucking Tuesday.\"got the club goin up tho \n",
        "descr=18 ~ I ain't got no type, bad bitches is the only thang that i like \n",
        "name=Kenneth Lamar \n",
        "Top Terms:\n",
        "d=got -0.229365390603\n",
        "fucking -0.159247936063\n",
        "d=bitches -0.109875560237\n",
        "d=ain't -0.0901759111148\n",
        "d=like -0.0815023168539\n",
        "the -0.0534844656456\n",
        "up -0.0256775536979\n",
        "d=18 -0.0209126163609\n",
        "d=the -0.0133272998407\n",
        "\"THIS_IS_A_MENTION 0.0265365768756\n",
        "d=that 0.0513508823513\n",
        "d=thang 0.0862957406748\n",
        "d=i 0.0865988954007\n",
        "tho 0.100519183517\n",
        "d=is 0.127503382315\n",
        "d=~ 0.294056067672\n",
        "it's 0.337116157443\n",
        "only 0.469014177376\n",
        "d=no 0.475961874721\n",
        "d=bad 0.49356152175\n",
        "goin 0.519509380707\n",
        "club 0.653456959193\n",
        "d=only 0.737124441791\n",
        "\n",
        "\n",
        "pred=1 (0.950452) truth=0 \n",
        "text=Go ahead and tell me there are better TV themes than Sanford &amp; Son and Speed Racer...GO AHEAD AND DO IT. \n",
        "descr=University of Tennessee Special Ed Major Pride of the Southland Band member and University of Tennessee Ambassador \n",
        "name=Samuel Gilliam\n",
        "Top Terms:\n",
        "d=tennessee -0.333599507364\n",
        "d=of -0.278956949134\n",
        "d=and -0.156067253542\n",
        "better -0.12565196883\n",
        "d=special -0.125406155617\n",
        "tell -0.0892689096561\n",
        "it. -0.0467785393718\n",
        "d=the -0.0133272998407\n",
        "d=major -0.0119003489473\n",
        "do 0.000574156846851\n",
        "tv 0.0127594887733\n",
        "and 0.0260745372736\n",
        "&amp; 0.141210649384\n",
        "there 0.162894517741\n",
        "d=ed 0.178698864824\n",
        "go 0.191941988955\n",
        "than 0.196616066501\n",
        "son 0.208627740424\n",
        "d=ambassador 0.230272427298\n",
        "d=pride 0.303531999704\n",
        "me 0.359690531198\n",
        "are 0.398927668573\n",
        "d=band 0.478154766601\n",
        "d=member 0.504766092624\n",
        "d=university 0.676744266446\n",
        "\n",
        "\n",
        "pred=1 (0.955457) truth=0 \n",
        "text=Troy's view on canceling classes when there's bad weather http://t.co/Lrsf5rhQQb \n",
        "descr=Never alone. 20. Troy University Sophomore. Jeremiah 29:11. #TroyU17 \n",
        "name=Jordan Rich\n",
        "Top Terms:\n",
        "d=sophomore. -0.267272086176\n",
        "on -0.118640765985\n",
        "d=alone. -0.0731605558722\n",
        "THIS_IS_A_URL 0.0173468434062\n",
        "bad 0.0277993270268\n",
        "view 0.0454006543123\n",
        "canceling 0.0554275458507\n",
        "d=jeremiah 0.145080786799\n",
        "classes 0.155776509365\n",
        "weather 0.18790810348\n",
        "there's 0.296659324182\n",
        "d=never 0.488713551399\n",
        "d=20. 0.547317156601\n",
        "when 0.654548137388\n",
        "d=university 0.676744266446\n",
        "\n",
        "\n",
        "pred=0 (0.970474) truth=1 \n",
        "text=YouDo! And you do it very well. ThankYou @stevekrohn @cindycapo @greensboro_nc @at_Randy @SIBREA @tripletsfan19 @RT2EAT #Leadership \n",
        "descr=on face book as http//Cindy http://Capobianco.Facebook.com, Instagram as capochino 67 NewFollowers no dm please!! #NOBLEBOSSFAMILY \n",
        "name=Cindy Capobianco\n",
        "Top Terms:\n",
        "very -0.684887592336\n",
        "d=THIS_IS_A_URL -0.591627325965\n",
        "it -0.386987451111\n",
        "d=as -0.318572369666\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        "d=book -0.154780532263\n",
        "d=on -0.103115415443\n",
        "do 0.000574156846851\n",
        "and 0.0260745372736\n",
        "you 0.0903342624185\n",
        "d=instagram 0.116567669425\n",
        "d=dm 0.18208056627\n",
        "d=no 0.475961874721\n",
        "\n",
        "\n",
        "pred=1 (0.993047) truth=0 \n",
        "text=@AmandaBoo_Xo @toddcarey commented person above u then skipped you and went to the next person :( okay I feel hurt Put yourself in my shoes \n",
        "descr=Photographer, I \u2764\ufe0f Todd Carey and Leo Cody! \u270c\ufe0fCali Chick \u2600\ufe0f 4/27/14 \u2764\ufe0f \n",
        "name=Sam Vaught\n",
        "Top Terms:\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        ":( -0.179730890562\n",
        "d=and -0.156067253542\n",
        "then -0.134829415183\n",
        "in -0.121804372095\n",
        "u -0.0583817191763\n",
        "the -0.0534844656456\n",
        "yourself -0.0418556363813\n",
        "d=photographer, -0.03786040257\n",
        "to -0.0291718973355\n",
        "next -0.0198162817384\n",
        "and 0.0260745372736\n",
        "hurt 0.0316208475792\n",
        "d=carey 0.0333239808445\n",
        "d=i 0.0865988954007\n",
        "you 0.0903342624185\n",
        "d=chick 0.182499149937\n",
        "my 0.183124635701\n",
        "put 0.229970752274\n",
        "feel 0.258997602187\n",
        "i 0.264195051906\n",
        "went 0.304717354715\n",
        "okay 0.461975310276\n",
        "person 0.483315496845\n",
        "shoes 0.505049260642\n",
        "d=\u2764\ufe0f 0.709777584933\n",
        "d=\u2600\ufe0f 0.941029506796\n",
        "\n",
        "\n",
        "pred=1 (0.963032) truth=0 \n",
        "text=Can I go home now? \"No Isaac, no you can't\". *guess the movie that quote is from. \n",
        "descr=Work hard. Be Humble. The rest is up to God. \n",
        "name=Isaac Pandian\n",
        "Top Terms:\n",
        "d=god. -0.465775091509\n",
        "that -0.241064645848\n",
        "d=to -0.162473602973\n",
        "\"no -0.156459576034\n",
        "from. -0.0862156236836\n",
        "the -0.0534844656456\n",
        "d=the -0.0133272998407\n",
        "d=hard. -0.00627898864734\n",
        "d=up 0.00728637614157\n",
        "d=humble. 0.0389706814448\n",
        "now? 0.0564941048219\n",
        "you 0.0903342624185\n",
        "no 0.0934813573648\n",
        "can 0.109875817208\n",
        "d=is 0.127503382315\n",
        "quote 0.176195356925\n",
        "go 0.191941988955\n",
        "is 0.234279985975\n",
        "i 0.264195051906\n",
        "d=work 0.308656382611\n",
        "movie 0.337331145952\n",
        "d=rest 0.499797894999\n",
        "home 0.81741114463\n",
        "d=be 0.865270812687\n",
        "\n",
        "\n",
        "pred=1 (0.982073) truth=0 \n",
        "text=I love being tall \n",
        "descr=so you think you're lonely, you think that you're the only one \n",
        "name=Juan F. Bermejo S.\n",
        "Top Terms:\n",
        "d=the -0.0133272998407\n",
        "tall 0.0215432700475\n",
        "d=you 0.0343911157781\n",
        "d=that 0.0513508823513\n",
        "being 0.208456439621\n",
        "d=so 0.2154050297\n",
        "love 0.223494633392\n",
        "d=you're 0.242031950848\n",
        "i 0.264195051906\n",
        "d=one 0.468518051711\n",
        "d=think 0.523837627761\n",
        "d=only 0.737124441791\n",
        "\n",
        "\n",
        "pred=0 (0.982211) truth=1 \n",
        "text=@ParksKMBZ it's not a bad idea - so many food allergies - have the cake at your party - plus parent competition - yuck \n",
        "descr=Mom of three fabulous kids - Razorback & KC sports fan - proud liberal - fan of bearded ginger haired men! \n",
        "name=janet smith\n",
        "Top Terms:\n",
        "d=sports -0.97683260452\n",
        "at -0.744185542548\n",
        "d=kc -0.62681117134\n",
        "- -0.540521319504\n",
        "party -0.365025744503\n",
        "d=liberal -0.351189056163\n",
        "idea -0.339601363569\n",
        "d=- -0.310579006536\n",
        "d=of -0.278956949134\n",
        "d=haired -0.252585439162\n",
        "plus -0.236072924026\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        "d=kids -0.181145861202\n",
        "d=three -0.159750723001\n",
        "a -0.128750878758\n",
        "d=bearded -0.0670177497216\n",
        "your -0.0663514974592\n",
        "the -0.0534844656456\n",
        "bad 0.0277993270268\n",
        "cake 0.0322985299601\n",
        "d=razorback 0.0590595106706\n",
        "allergies 0.0859185161313\n",
        "many 0.117843931471\n",
        "food 0.146000271771\n",
        "have 0.213284201436\n",
        "d=ginger 0.262702490534\n",
        "d=& 0.267185154506\n",
        "so 0.294977454705\n",
        "d=fan 0.31593953605\n",
        "it's 0.337116157443\n",
        "d=mom 0.342320582256\n",
        "d=proud 0.636316829846\n",
        "not 0.717067705818\n",
        "\n",
        "\n",
        "pred=1 (0.953633) truth=0 \n",
        "text=Or not \ud83d\ude22 good rip though \n",
        "descr=Our dark pasts can be used as our greatest asset towards a bright future. \n",
        "name=Alec Hacker\n",
        "Top Terms:\n",
        "good -0.541432857837\n",
        "\ud83d\ude22 -0.351043799014\n",
        "d=as -0.318572369666\n",
        "d=towards -0.210910916685\n",
        "d=used -0.0439597783806\n",
        "or -0.0376443604887\n",
        "d=future. 0.0494345626017\n",
        "d=a 0.0515703700588\n",
        "d=dark 0.0930984799477\n",
        "d=can 0.166230114758\n",
        "rip 0.282430090944\n",
        "though 0.462328316262\n",
        "not 0.717067705818\n",
        "d=our 0.80685965797\n",
        "d=be 0.865270812687\n",
        "\n",
        "\n",
        "pred=1 (0.98169) truth=0 \n",
        "text=I love being 6'3 \n",
        "descr=so you think you're lonely, you think that you're the only one \n",
        "name=Juan F. Bermejo S.\n",
        "Top Terms:\n",
        "d=the -0.0133272998407\n",
        "d=you 0.0343911157781\n",
        "d=that 0.0513508823513\n",
        "being 0.208456439621\n",
        "d=so 0.2154050297\n",
        "love 0.223494633392\n",
        "d=you're 0.242031950848\n",
        "i 0.264195051906\n",
        "d=one 0.468518051711\n",
        "d=think 0.523837627761\n",
        "d=only 0.737124441791\n",
        "\n",
        "\n",
        "pred=1 (0.981152) truth=0 \n",
        "text=@andreayvetteg do you want me to get you more beef? \n",
        "descr=Don't chase your dreams, catch em. Working out is life. UTPA 18. Do what you love, not what they love. Family first. \n",
        "name=Bobby Fratt\n",
        "Top Terms:\n",
        "d=they -0.434716328764\n",
        "d=not -0.321298144387\n",
        "THIS_IS_A_MENTION -0.208574066089\n",
        "get -0.166866513229\n",
        "d=do -0.149276408666\n",
        "d=working -0.126431644331\n",
        "d=catch -0.0644982684637\n",
        "to -0.0291718973355\n",
        "do 0.000574156846851\n",
        "d=you 0.0343911157781\n",
        "d=first. 0.0470803799056\n",
        "d=chase 0.0483415045829\n",
        "more 0.0641646819901\n",
        "you 0.0903342624185\n",
        "d=love, 0.126956074825\n",
        "d=is 0.127503382315\n",
        "d=dreams, 0.1505555614\n",
        "d=don't 0.155061307096\n",
        "want 0.170624627072\n",
        "d=family 0.211061277578\n",
        "d=what 0.310553213443\n",
        "me 0.359690531198\n",
        "d=out 0.384802986499\n",
        "d=18. 0.475394082772\n",
        "d=life. 0.53202545457\n",
        "d=your 0.541063195391\n",
        "d=love. 0.99600941518\n",
        "\n",
        "\n",
        "pred=1 (0.980898) truth=0 \n",
        "text=Sad part is I knew what I was getting myself into when I told this nigga I'll bring him fml. \n",
        "descr=None \n",
        "name=Garth frederick\n",
        "Top Terms:\n",
        "told -0.531125169344\n",
        "this -0.132274360559\n",
        "part -0.128768733732\n",
        "getting -0.0969259970667\n",
        "knew -0.0678133091206\n",
        "was -0.0441849406017\n",
        "bring 0.058950653309\n",
        "nigga 0.0995710472027\n",
        "what 0.172881360463\n",
        "is 0.234279985975\n",
        "i 0.264195051906\n",
        "i'll 0.416499581162\n",
        "him 0.424624914227\n",
        "sad 0.444764789542\n",
        "into 0.465346308816\n",
        "when 0.654548137388\n",
        "myself 0.949622843835\n",
        "\n",
        "\n",
        "pred=1 (0.970617) truth=0 \n",
        "text=Can someone give me a header to match my icon? Please lol I'm desperate \n",
        "descr=Waiting on your bloodlines \n",
        "name=Joe Parsons\n",
        "Top Terms:\n",
        "lol -0.37574216996\n",
        "a -0.128750878758\n",
        "d=on -0.103115415443\n",
        "to -0.0291718973355\n",
        "can 0.109875817208\n",
        "match 0.146899439076\n",
        "my 0.183124635701\n",
        "i'm 0.223662362206\n",
        "d=waiting 0.255751147968\n",
        "me 0.359690531198\n",
        "give 0.390439773399\n",
        "d=your 0.541063195391\n",
        "someone 0.797139476306\n",
        "please 0.900558915109\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 168
    }
   ],
   "metadata": {}
  }
 ]
}